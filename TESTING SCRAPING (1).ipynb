{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Web%20site%20structure.png](attachment:Web%20site%20structure.png?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing page with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data from website and create a BeautifulSoup class to parse document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists where store the data\n",
    "title=[]\n",
    "price=[]\n",
    "n_room=[]\n",
    "mq=[]\n",
    "n_bathroom=[]\n",
    "plan=[]\n",
    "desc=[]\n",
    "i=1\n",
    "#iterate to take data from different page\n",
    "for num in tqdm(range(1,1728)):\n",
    "    url = str('www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag='+str(1))\n",
    "    r = requests.get('https://' +url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,'html.parser')\n",
    "\n",
    "    #iterate on the main class which contain all the data \n",
    "    for announcement in tqdm(soup.find_all('div', class_='listing-item_body--content')):\n",
    "        #find all 'li' class which contain the data that we want and put them on the lists\n",
    "        feature = announcement.find_all('li')\n",
    "        if len(feature)>=6:\n",
    "            #filter word on price\n",
    "            feature_price = feature[0].text.strip()\n",
    "            if ('PREZZO SU RICHIESTA' in feature_price )== True:\n",
    "                break\n",
    "            elif('%' in feature_price) == True:\n",
    "                feature_price = feature_price.replace('.','').replace('€','').replace(' ','')\n",
    "                price.append(feature_price[-19:-12])\n",
    "            else:\n",
    "                feature_price = feature_price.replace('.','').replace('€','').replace(' ','')\n",
    "                price.append(feature_price)\n",
    "            feature_n_room = feature[1].text[0]\n",
    "            n_room.append(feature_n_room)\n",
    "            feature_mq = feature[2].text[0:-12].replace('\\xa0','')\n",
    "            mq.append(feature_mq)\n",
    "            feature_n_bathroom = feature[3].text[0:-5].replace('+','').replace('\\xa0','')\n",
    "            n_bathroom.append(feature_n_bathroom)\n",
    "            feature_plan = feature[4].text[0:-7].strip()\n",
    "            #check if there is a character inside plan data, if yes pop last element in the others lists to mantain dimension\n",
    "            if feature_plan.isalpha()==True:\n",
    "                #title.pop()\n",
    "                price.pop()\n",
    "                n_room.pop()\n",
    "                mq.pop()\n",
    "                n_bathroom.pop()\n",
    "                i+=0\n",
    "\n",
    "            else:    \n",
    "                plan.append(feature_plan)\n",
    "\n",
    "                #find <a element where <href is present\n",
    "                link=announcement.find('a', href=True)\n",
    "                #check to solve the problem where the <href didn't have the complete link\n",
    "                if ('https://' in link['href'])==True:\n",
    "                    ann=requests.get(link['href'])\n",
    "                else:\n",
    "                    ann=requests.get('https://www.immobiliare.it'+link['href'])\n",
    "                data1 = ann.text\n",
    "                #create another bs4 element to access in the specific link and take commplete text from announcement\n",
    "                soup1 = BeautifulSoup(data1,'html.parser')\n",
    "                #access to link and take complete text\n",
    "                a=soup1.find('div', attrs={'role':'contentinfo'}).text.strip()\n",
    "                desc.append(a)\n",
    "                title.append('announcement_'+str(i))\n",
    "                i+=1\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(title))\n",
    "print(len(price))\n",
    "print(len(n_room))\n",
    "print(len(mq))\n",
    "print(len(n_bathroom))\n",
    "print(len(plan))\n",
    "print(len(desc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oNow we store the data in a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table=pd.DataFrame({'Annuncio':title,\n",
    "                    'Prezzo':price,\n",
    "                    'Camere':n_room,\n",
    "                    'Superficie':mq,\n",
    "                    'Bagni':n_bathroom,\n",
    "                    'Piano':plan})\n",
    "#used to remove a \\n inside \"Piano\" coloumn\n",
    "table =table.set_index('Annuncio')\n",
    "table=table.replace({r'\\n': '',r'\\+' : '',r'\\xa0': ''}, regex=True)\n",
    "table.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take all the announcement from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only see if the dataframe is the data are correct, then i remove this\n",
    "annuncio=pd.DataFrame({'Annuncio':desc})\n",
    "annuncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to preprocess all text in the announcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    # removing '\\n'\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    # removing punctuation\n",
    "    tokenizer = regexp_tokenize(text, \"[\\w\\$]+\")\n",
    "    # filter the non stopwords\n",
    "    filtered = [w for w in tokenizer if not w in stopwords.words('italian')]\n",
    "    ps = PorterStemmer()\n",
    "    # removing the stem\n",
    "    filtered = [ps.stem(word) for word in filtered]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre process all the announcement and put in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_list=[]\n",
    "for i in desc:\n",
    "    processed_list.append(preprocess(i))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have a list of word processed, now i convert it to a list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[' '.join(i) for i in processed_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ren=[int(i) for i in range(1,len(b)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N.B.need to find how to increse the docId in relation of document!!!!!!!!!\n",
    "#creating dataframe with wordId and all text from the announcement processed\n",
    "df1 = pd.DataFrame({'wordId': ren, \n",
    "               'parole': b})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have used pandas and numpy to compute Tf-IDF in all dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize and generate count vectors\n",
    "word_vec = df1.parole.apply(str.split).apply(pd.value_counts).fillna(0)\n",
    "# Compute term frequencies\n",
    "tf = word_vec.divide(np.sum(word_vec, axis=1), axis=0)\n",
    "# Compute inverse document frequencies\n",
    "idf = np.log10(len(tf) / word_vec[word_vec > 0].count())\n",
    "# Compute TF-IDF vectors\n",
    "tfidf = np.multiply(tf, idf.to_frame().T)\n",
    "#L2 (Euclidean) normalization\n",
    "l2_norm = np.sum(np.sqrt(tfidf), axis=1)\n",
    "#Normalized TF-IDF vectors\n",
    "tfidf_norm = (tfidf.T / l2_norm).T\n",
    "#put in a dataframe\n",
    "#second_mat=pd.DataFrame(tfidf_norm)\n",
    "#word_column=['word_'+str(i) for i in range(1,len(second_mat.columns)+1) ]\n",
    "#second_mat.columns=word_column\n",
    "#second_mat=second_mat.set_index(table.index)\n",
    "#second_mat.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = df1.parole.apply(str.split).apply(pd.value_counts)#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_mat=pd.DataFrame(tfidf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_mat.to_csv(r'C:\\Users\\Daniele\\Desktop\\Matrix_TfIdf.csv')\n",
    "table.to_csv(r'C:\\Users\\Daniele\\Desktop\\Matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=pd.read_csv(r'C:\\Users\\Daniele\\Desktop\\Matrix.csv')\n",
    "table=b\n",
    "table=table.set_index(['Annuncio'])\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv(r'C:\\Users\\Daniele\\Desktop\\Matrix_TfIdf.csv')\n",
    "second_mat=a\n",
    "second_mat=second_mat.set_index(['Annuncio'])\n",
    "second_mat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to matrix\n",
    "dataset_array =second_mat.values\n",
    "dataset_array1=table.values\n",
    "# Using sklearn\n",
    "km =KMeans(n_clusters=5, init='k-means++', tol=0.0001).fit(dataset_array)\n",
    "km1=KMeans(n_clusters=5, init='k-means++', tol=0.0001).fit(dataset_array1)\n",
    "# Get cluster assignment labels\n",
    "labels = km.labels_\n",
    "labels1 = km1.labels_\n",
    "#put the result of cluster in a dataframe\n",
    "results = pd.DataFrame([second_mat.index,labels]).T\n",
    "results1 = pd.DataFrame([table.index,labels]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=pd.merge(results, results1, on=0)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection / union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1=km.cluster_centers_\n",
    "cluster2=km1.cluster_centers_    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km1.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cluster1)):\n",
    "    print(jaccard_similarity(cluster1[i],cluster2[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the duplicates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(km1.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
